---
title: "Reframing Canada's Employment Insurance Modernization Through Generative Research"
slug: "project-3"
role: "Lead Generative Researcher (Pod Lead)"
org: "Employment & Social Development Canada (via Deloitte)"
timeline: "2024 (multi-month engagement)"
team: "12+ researchers across 3 pods — Accenture, Deloitte, CGI, and ESDC collaborating"
tools: ["Dovetail", "Generative Interviews", "Journey Mapping", "Synthesis Frameworks", "Policy Analysis"]
tags: ["Generative Research", "Service Design", "Systems Thinking", "Policy", "Federal Government"]
impactHeadline: "Reframed Canada's largest EI modernization effort — from a UX confusion problem to a policy–system misalignment problem — shifting how a multi-billion-dollar federal program prioritized its redesign roadmap."
angle: "Structural reframing under constraint inside a politically sensitive federal system"
---

## TL;DR

Employment & Social Development Canada launched one of the largest service redesign efforts in federal government history — modernizing Employment Insurance for millions of Canadians. I was hired as a junior researcher and promoted mid-engagement to Pod Lead when the team lacked synthesis direction. The original mandate was tactical: understand how Canadians combine and share EI benefits. What I uncovered was structural: the real barrier wasn't confusing UX — it was the invisible, uncrossed boundary between policy intent and system logic. I formalized that as the policy–system boundary model, built the evidence architecture to support it, and used it to navigate "policy is out of scope" resistance while keeping the findings intact. Solution Strategy shifted from treating transitions as content problems to treating them as system orchestration problems. Joint parental benefit concepts entered concept testing. The research archive became the foundation for future service design sprints.

---

## Context

- EI is relied on by millions of Canadians during financial instability — disability, job loss, parental leave, illness
- The modernization mandate covered "combining" (transitioning between benefit types) and "sharing" (e.g., parental benefits split between parents)
- Research deliberately centered underserved communities: remote and rural Canadians, Indigenous clients, newcomers, migrant workers, caregivers, fishers, elderly Canadians, people with disabilities, people with low digital literacy
- 12+ researchers across 3 pods; 4 organizations collaborating (Accenture, Deloitte, CGI, ESDC)
- Politically sensitive at the federal level — research directly influenced multi-year, multi-billion-dollar program decisions
- Legacy EI systems had no data integration across benefit types; policy rules were opaque, contradictory, and filled with jargon even to staff
- Promoted internally from junior researcher to Pod Lead mid-engagement when the initial lead lacked structural synthesis direction

---

## Problem

The system had no clear answer for Canadians trying to transition between benefit types — sickness to maternity, maternity to parental, parental to other benefits. Many Canadians failed to receive benefits they had paid into, not because of eligibility issues, but because the experience of navigating transitions was too broken to complete.

The surface diagnosis was "confusing UX." The actual problem ran deeper.

As I mapped transitions through interviews and SME sessions, the same pattern kept surfacing: participants weren't just confused by flows — they were confused by contradictions between policy intent, system rules, and operational interpretation. The same transition could be explained three different ways by three different ESDC employees, because the system itself did not enforce a consistent answer.

**The reframe**: this wasn't a UX problem. It was a policy–system misalignment problem — and those two things required completely different interventions. Until that distinction was made explicit, no design work could be correctly scoped.

**Stakes**: a federal program at a breaking point, causing internal rework, processing delays, inconsistent decisions, and client harm. Shared parental benefits — jointly owned money — were being applied for separately, creating overpayments, delayed payments, and investigation triggers. The program was about to scale modernization without a clear diagnosis of what actually needed fixing.

---

## What I Did

- **Stepped into the Pod Lead role mid-engagement** when synthesis direction was absent — restructured areas of inquiry, rebuilt interview guides to sharpen around benefit-to-benefit transitions, and shifted the team from collecting insights to building decision infrastructure
- **Built the Dovetail tagging architecture** from scratch: CX themes, EX themes, cross-benefit transition tagging layer, and a unified pain point structure linking evidence to KPIs — creating a searchable, traceable archive rather than a flat insight dump
- **Formalized the CX/EX split** so the research could show cause-and-effect between what clients experience and what employee constraints cause it — revealing the system-side origins of client failure, not just the symptoms
- **Surfaced the policy–system boundary** as an explicit structural model: mapping what is legislated and immovable (policy) vs. what is configurable and redesignable (system logic) vs. where misalignment between the two creates failure — giving Solution Strategy a precise scoping tool
- **Navigated the "policy is out of scope" constraint surgically**: instead of critiquing policy, I built a visual boundary model that respected legislative immovability while clearly identifying system-level redesign opportunities — keeping the insights intact without triggering political resistance
- **Documented the transition failure modes** specifically: no clear conversion path between benefit types, no joint application mechanism for shared parental benefits, no system-enforced guidance for transitions — turning vague "confusion" into precise, addressable system gaps
- **Produced and curated executive-ready share-outs**, including live video clip packages from participant sessions — creating direct emotional evidence for leadership that abstract frameworks couldn't produce
- **Delivered the full research output**: Generative Research Report, CX + EX Journey Maps, Insight Frameworks, Opportunity Area Maps, a cross-benefit Transition Pathways model, KPI-linked recommendations, and a policy–system boundary model handed to the Solution Strategy team

---

## Key Decisions & Tradeoffs

**Chose to reframe the mandate before finalizing the research design.**
The original framing ("how do Canadians combine and share benefits") would have produced a UX audit of a broken system. I pushed to treat the transitions as a lens into systemic failure, not just a usability problem. Cost: a more complex synthesis and a harder story to tell to stakeholders who wanted concrete UI fixes. Benefit: findings that were actually decision-useful at program level.

**Chose a surgical framing for the policy constraint rather than confronting it.**
"Policy is out of scope" was a real political boundary. Pushing against it directly would have gotten findings buried. Instead, I built the boundary model to do two things simultaneously: respect what couldn't be changed, and make visible what could. This kept the research in scope while changing what "in scope" meant in practice.

**Chose evidence architecture over insight volume.**
The instinct in large generative studies is to collect everything. I made the opposite call: build a tagging system that makes evidence traceable and decision-linked from the start. This cost time upfront but meant the archive was usable by teams that came after us — not just by the people who built it.

**Chose to separate CX and EX as parallel threads deliberately.**
Most research programs treat client experience as the subject and employee experience as context. I ran them as co-equal threads, mapping the causal relationship between them. This was structurally more complex but produced the only credible explanation for why client failures were happening — and where intervention would actually land.

---

## Results

- **Solution Strategy shifted its framing**: transitions stopped being treated as content or UX problems and started being treated as system orchestration problems — a directional change that affected feasibility assessments and roadmap sequencing
- **Joint parental benefit concepts entered concept testing** as a direct result of the shared benefits transition findings — a new design direction that hadn't been on the roadmap before the research surfaced the overpayment and misallocation failure modes
- **Transition pathways became a formal roadmap conversation** instead of an edge-case discussion — elevated from "too complex to address" to a prioritized design problem with a defined scope
- **The research archive became the reference source** for future service design sprints — structured evidence base, traceable from insight to implication to opportunity, linked to the program's Value Management Framework KPIs
- **The policy–system boundary model gave Solution Strategy a precision tool** for evaluating what could and couldn't be changed — replacing vague "this is policy" deferrals with a specific, visual boundary that design and technical teams could work from
- **Executive leadership reached a documented "now we get it" moment** when qualitative video clips were paired with the boundary model in a share-out — securing alignment on the human-centered direction for the program

---

## Artifacts

*(To be embedded during Day 8 component pass)*

- `p3-01-boundary-model.webp` — Policy vs. system boundary diagram (the core reframe artifact)
- `p3-02-cx-ex-journey.webp` — Parallel CX + EX journey map showing cause-and-effect
- `p3-03-transition-pathways.webp` — Cross-benefit transition pathways model
- `p3-04-tagging-architecture.webp` — Dovetail tagging schema (CX/EX/transition layers)
- `p3-05-opportunity-map.webp` — Opportunity area map linked to KPIs

---

## What I'd Improve Next

- **Quantitative validation layer**: the generative phase was deliberately qualitative, but pairing key findings (especially the transition failure modes) with call centre data or processing error rates would have made the case for system-level intervention much harder to defer
- **Earlier boundary model introduction**: I built the policy–system model mid-synthesis. Introducing it as an explicit framework during the research design phase — not just as a synthesis output — would have sharpened interview guides further and reduced the need for retrospective reframing
- **More structured handoff protocol**: the archive and boundary model were handed to Solution Strategy, but the transition between generative research and concept testing was informal. A structured briefing process would have increased the fidelity of how findings were translated into design hypotheses
- **Clearer scope protection from the start**: being promoted mid-engagement to Pod Lead meant inheriting scope and team dynamics that were already partially set. Advocating for clearer research governance at the outset — not just fixing it after the fact — would have reduced early-phase insight sprawl

---

## Meta Add-On

**Why this case is structurally different from CS1 and CS2:**
CS1 proves architecture thinking. CS2 proves strategic alignment. CS3 proves you can operate in a system where the problem is politically constrained, the stakes are real (millions of people, billions of dollars), and the work you produce has to survive contact with a program that will outlast your involvement. That's a different level of professional maturity — and it's the thing worth leading with when this case study is the one that gets read.

**The credibility anchor for this case:**
Everything in the Results section is framed as directional change, not metric change — because that's what's honest. Generative research doesn't produce A/B test results. It produces reframes. The reframe here is concrete and traceable: a program changed how it scoped a problem because of this work. That's the claim. Don't inflate it. Don't shrink it.

**The asset that would most strengthen this case:**
The boundary model diagram. It's the artifact that makes the abstract concrete. If you can produce even a rough reconstruction of the policy/system/misalignment visual — three zones, labeled clearly — it anchors everything else in the case study and gives a recruiter something to look at that explains the insight without needing to read four paragraphs.