---
title: "Defining the CX North Star for a Multi-Year Digital Modernization Program"
role: "Lead User Researcher & UX Strategist"
org: "Pacific Blue Cross (via Deloitte)"
timeline: "2024 (~12-week foundational phase, multi-year program)"
team: "Lead UX Strategist + ELT + product owners across 8 workstreams + CRM/platform/digital technical leads"
tools: ["Executive Alignment Workshops", "Qualitative Research", "Service Blueprinting", "Miro", "Future-State Visioning", "Journey Mapping", "Quantitative Survey"]
tags: ["CX Strategy", "Service Design", "Systems Thinking", "Enterprise Alignment", "Research"]
impactHeadline: "Anchored a multi-year digital modernization program in human-centered CX strategy — producing 4 future-state Experience Blueprints across all user segments that became the source-of-truth for cross-functional implementation teams."
angle: "Enterprise CX orchestration — from fragmented tech-led workstreams to a unified, evidence-backed experience architecture"
---

## TL;DR

Pacific Blue Cross launched a multi-year digital modernization program while ranking last in BC for technology for intermediaries and 2nd last for service to plan sponsors — but without a unified CX vision to guide eight parallel workstreams. I pushed to halt tech-led execution and anchor the program in a human-centered North Star first. Over a structured 12-week engagement, I led executive alignment workshops, ran qualitative research across members, advisors, and two sponsor segments, commissioned quantitative surveys of 200 emerging sponsor organizations and 501 individual customers, and co-created 4 future-state Experience Blueprints spanning every user segment. Those blueprints became the source-of-truth artifact for all downstream implementation decisions.

---

## Context

- Pacific Blue Cross: major BC-based health benefits provider, ranked **last in BC for technology for intermediaries** and **2nd last for service to plan sponsors** (NMG Consulting benchmarking)
- 8 parallel modernization workstreams operating independently — technology-led, no shared experience vision
- **44% of all call centre cases** were claims status inquiries — the single highest call driver, indicating a systemic transparency failure
- **66% of VoC respondents** reported call centre agents didn't have the information they needed
- Highly divergent customer segments: large strategic sponsors (unions), emerging small business sponsors, individual members, financial advisors — each with structurally incompatible expectations
- Legacy systems creating manual workarounds across advisor, sponsor, and member touchpoints
- No enterprise CX strategy, no experiential KPIs, no cross-workstream alignment mechanism at program start

---

## Problem

Eight workstreams were modernizing in parallel without a shared answer to a fundamental question: what should the future Pacific Blue Cross experience actually feel like?

The benchmarking data made the stakes explicit: PBC was losing ground on the metrics that matter most to its intermediary and sponsor channels, while internally, the most common reason customers called — claims status — was a pure information access failure, not a claims processing failure. The system was generating avoidable contact volume because it didn't surface the right information at the right time.

The structural risk was precise: technology modernization without a human-centered CX vision produces faster, more expensive versions of the same fragmented experience. Each workstream would optimize locally while the end-to-end customer journey remained broken at the seams.

Two compounding factors made the risk acute:
- **No experiential KPIs**: workstream success was measured by technical delivery milestones, not customer outcomes
- **CRM as a missing dependency**: personalization was a stated modernization ambition, but no workstream had mapped CRM enablement as a prerequisite — meaning personalization would be built without the infrastructure to support it

---

## What I Did

- **Reframed the program mandate** before execution advanced: identified that 8 tech-led workstreams without a unified CX vision would deliver infrastructure improvements while preserving experience fragmentation — secured executive sponsorship to prioritize CX strategy formation first
- **Led executive alignment workshops** (CX Vision Workshop, ELT sessions) translating PBC's brand promise ("Inspire Confidence in Health") into 3 actionable CX vision attributes and guiding principles, creating the strategic anchor for all workstreams
- **Ran qualitative research across 4 user segments**: members, financial advisors, strategic sponsors (large unions), and emerging sponsors (small businesses) — surfacing structurally different pain points, moments that matter, and future-state expectations
- **Commissioned and analyzed quantitative research**: a 46-question survey of 200 emerging sponsor organizations across BC, and a 501-respondent individual customer survey — grounding the future-state blueprints in measurable data, not only qualitative insight
- **Ran future-state vision workshops with product owners** across workstreams — translating research findings and CX attributes into concrete experience models structured around differentiators, table stakes, and technology enablers
- **Produced 4 distinct future-state Experience Blueprints** covering every major user segment: Group Members, Strategic Sponsors, Emerging Sponsors, and Individual (Pre-Purchase) — each mapping frontstage and backstage actions, engagement channels, capability requirements, and moments of differentiation vs. table stakes
- **Defined experiential KPIs** tied to the CX vision attributes across all workstreams — ensuring measurement was anchored in customer outcomes, not technical delivery
- **Mapped CRM as a required dependency** for personalization ambitions — surfacing a critical program gap no workstream had explicitly owned, embedding it into the technical roadmap
- **Produced initiative charters** handed to implementation teams, defining strategic imperatives, functional scope, and sequencing for downstream delivery

---

## Key Decisions & Tradeoffs

**Chose to halt and reframe before execution advanced.**
The program had momentum toward workstream execution. I introduced deliberate friction — insisting on ELT alignment and a CX North Star before roadmap prioritization. The cost: slowed early execution. The payoff: 8 workstreams with a shared strategic anchor instead of 8 teams optimizing against different implicit assumptions.

**Chose quantitative research alongside qualitative, not instead of.**
A 12-week engagement could have stayed qualitative for speed. I pushed to add a 200-org emerging sponsor survey and 501-respondent individual survey because the emerging sponsor segment was a stated growth priority — and strategic decisions about distribution channels and product differentiation needed to be grounded in measurable data, not interviews alone.

**Chose 4 distinct blueprints over one unified blueprint.**
A single unified blueprint would have been faster to produce but would have obscured the structurally different needs of strategic vs. emerging sponsors, and group members vs. individual customers. Four distinct blueprints increased production complexity but gave implementation teams segment-specific guidance — preventing future design decisions from defaulting to "average user" thinking.

**Chose to surface CRM as a missing dependency, not stay in scope.**
CRM enablement wasn't in my explicit mandate. I flagged it as a critical gap — without it, personalization couldn't be delivered regardless of what the blueprints specified. Raising it created complexity in the program plan but prevented a scenario where personalization features were built without the infrastructure to support them.

**Chose Experience Blueprints over Journey Maps as the primary artifact.**
Journey maps describe experience from the customer's perspective. Experience blueprints additionally show the backstage: PBC actions, capability requirements, and system dependencies that must exist to deliver each frontstage moment. That distinction matters in a modernization program — it connects customer experience to implementation scope, giving technical workstreams a clear line of sight from "what the customer experiences" to "what we need to build."

---

## Results

- **4 future-state Experience Blueprints delivered** across all user segments — Group Members, Strategic Sponsors, Emerging Sponsors, and Individual Pre-Purchase customers; each became the source-of-truth for cross-functional teams and initiative planning
- **CX strategy formalized**: brand promise + 3 CX vision attributes (Know & Care for Me, Simplify the Complex, Reliable Every Time) + guiding principles + objectives and KPIs — providing the governance framework for all 8 workstreams
- **Experiential KPIs defined** across all workstreams — shifting measurement from technical delivery milestones to customer outcome metrics
- **200-org quantitative survey and 501-respondent individual survey** produced — grounding growth segment strategy (emerging sponsors, women, near-retirees) in measurable data
- **Initiative charters produced and handed off** to implementation teams — defining strategic imperatives, functional scope, and sequencing for seamless claims, provider network enablement, sponsor self-service, and internal servicing capability
- **CRM dependency surfaced and embedded** in the technical roadmap — closing a program gap that would have blocked personalization delivery
- **Program-level scope expansion**: research and blueprint outputs surfaced a gap between the original PBC Thrive path and the target-state experience, prompting leadership to broaden the program's experience architecture scope

---

## Artifacts

*(To be embedded during Day 8 component pass)*

- `p2-01-cx-strategy-page.webp` — CX Strategy on a Page: brand promise, vision attributes, objectives, KPIs (sanitized)
- `p2-02-blueprint-anatomy.webp` — Experience Blueprint structure: frontstage, backstage, engagement channels, capability maturity (anonymized)
- `p2-03-research-pipeline.webp` — 3-phase research approach: Discover → Understand → Document
- `p2-04-segment-insights.webp` — Cross-segment insight summary: members, advisors, strategic sponsors, emerging sponsors
- `p2-05-initiative-charters.webp` — Initiative charter structure connecting future-state experience to implementation scope

---

## What I'd Improve Next

- **Stronger baseline measurement from the start**: the NMG benchmarking data existed but wasn't fully integrated into the KPI framework at program launch — entering with explicit "before" benchmarks would have made post-implementation measurement more precise
- **Earlier technical feasibility integration**: engineering leads were consulted but not embedded in vision workshops; bringing them in earlier would have caught CRM and system constraints before blueprint completion rather than at handoff
- **Longitudinal tracking mechanism**: initiative charters defined scope but didn't establish a formal measurement cadence; a quarterly KPI review process would have ensured the North Star remained a live governance tool post-handoff
- **More explicit prioritization scoring**: the Lead/Differentiate vs. Table Stakes distinction within blueprints was useful but informal; a structured scoring model (impact × feasibility × strategic fit) would have given implementation teams a cleaner sequencing tool

---

## Meta Add-On

**2 weak areas in the narrative:**
1. The initiative charters are your strongest handoff artifact but currently listed as a placeholder visual. The charter structure — strategic imperative, functional scope, sequencing — is exactly what separates "I did strategy" from "I handed implementation teams something actionable." A sanitized single-charter example would close this gap.
2. The NMG benchmarking (last in BC for technology, 2nd last for service to plan sponsors) appears only in Context. It should also frame the Results section as the "before" state — making the blueprint delivery feel like a response to a measurable competitive problem, not an internal program exercise.

**1 potential overclaim:**
"Program-level scope expansion" is accurate — the deck documents it explicitly ("the recommended approach is to broaden the original PBC Thrive path"). But attribute it precisely: research and blueprint outputs surfaced the gap; leadership made the pivot decision. You produced the evidence; you didn't make the call.

**1 suggestion to increase signal density:**
Add a Callout component in the Problem section showing the reframe: *"Original program framing: modernize technology across 8 workstreams. Reframed: define what the target experience must feel like — then modernize the technology to support it."* This is the same structural reframe as Manulife in a completely different domain. Two case studies showing the same thinking pattern reads as a signature, not a coincidence.